Algebra and Linear Algebra in AI

$$
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
+
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
=   \begin{bmatrix}
6 & 8 \\
10 & 12
\end{bmatrix}
$$

Algebra and linear algebra might sound like complicated math topics, but they are actually the backbone of many technologies shaping our world today, including Artificial Intelligence (AI), data analysis, and machine learning.

To make this real, imagine Alex, a data scientist. His goal is to build an AI system that can predict how a company’s stock price might move after news comes out.

Here’s how his system works:

- Step 1: the AI **analyzes the text** of the news and assigns a **sentiment score**, a  number that reflects how positive or negative the news is.

- Step 2: the AI **predicts** how much the stock price will rise or fall, based on that sentiment score.

But how does the AI make that jump from **input numbers (sentiment scores)** to **output predictions (price changes)**?
This is where algebra and linear algebra step in.


### Foundations: Expressions, Equations, and Inequalities

Before we dive into linear models or advanced systems, it is important to understand the **basic mathematical tools** that make everything possible.


#### **Expressions vs Equations**

Let’s break this down clearly.

 **Expression**
  An expression is a combination of numbers, variables, and mathematical operations (like +, -, ×, ÷).
  **Important:** An expression does not have an equals sign.

  **Example 1:**

$$
2x + 3
$$



  This is simply describing a relationship or pattern between $x$ and the result. You cannot “solve” an expression because there is no equation to solve.

  **Example 2:**

  $$
  5y^2 - 7
  $$

  Again, just a combination, no equality or solution.



 **Equation**
  An equation is when an expression is set equal to something.
  **Important:** Equations can be solved because they give a specific condition.

  **Example 1:**

  $$
  2x + 3 = 7
  $$

  Here, you can solve for $x$ because it asks: “For what value of $x$ does the left side equal the right side?”

  **Solution:**

  $$
  2x + 3 = 7 \implies 2x = 4 \implies x = 2
  $$

  **Example 2:**

  $$
  5y^2 - 7 = 18
  $$

  Here, you would solve for $y$ by first isolating the variable.



* Expressions help describe relationships, like how strongly sentiment might affect price change.
* Equations help us calculate **specific** predictions, like “if sentiment is +0.8, how much will the price rise?”



#### **Try it Yourself**

1. Is this an expression or an equation?
   $3z^2 + 4z - 5$
   

2. Solve for $x$ in the equation:
   $4x - 6 = 10$

3. Write an equation using a variable that models this sentence:
   “The predicted price change is three times the sentiment score plus 1 percent.”





#### **Inequalities**

Inequalities describe conditions, ranges, or boundaries, they tell us what values **can** or **cannot** happen, rather than giving an exact solution.

**Example 1:**

  $$
  x > 5
  $$

  This says $x$ can be any number **greater** than 5.

**Example 2:**

  $$
  y \leq 10
  $$

  This says $y$ can be **less than or equal to** 10.




### **Inequalities help define rules and limits. For example:**

* Only trigger a trade if the predicted price increase $> 2\%$.
* Avoid stocks where the predicted price drop $\leq -5\%$.
* Ensure the model’s risk exposure stays below a certain maximum.



#### **Practice Inequalities**

1. Which values satisfy the inequality $s < 0$?
   (

2. If the model predicts $p \geq 3$ percent, should you consider the trade according to a rule that only takes trades above 2 percent?

3. Write an inequality for this situation:
   “We will only invest if the expected profit is at least twice the expected loss.”



### Summary Table

| Concept        | What It Does                                                         | Example                     |
| -------------- | -------------------------------------------------------------------- | --------------------------- |
| **Expression** | Shows a relationship using variables and operations                  | $2x + 3$, $4y^2 - 1$        |
| **Equation**   | Sets an expression equal to something, so you can solve for unknowns | $2x + 3 = 7$, $y^2 - 1 = 8$ |
| **Inequality** | Sets a condition or limit on the values                              | $x > 5$, $y \leq 10$        |



### **Functions — Mapping Inputs to Outputs**

A **function** is like a machine:

* You provide an input $x$,
* The machine applies a rule $f(x)$,
* You get an output $y$.

---

| **Example Function** |
| -------------------- |
| $f(x) = 2x + 3$      |

If input $x = 1$:

$$
f(1) = 2 \times 1 + 3 = 5
$$

This kind of rule can represent how one variable (like a numerical score) affects another (such as predicted price movement).

Why is this useful in AI?
AI models often use functions — sometimes very complex or nonlinear — to transform raw input data (like text scores, prices, volumes) into useful predictions or classifications.

Without functions, we cannot systematically map input patterns to meaningful outputs.



### Apply a Simple Function

Given $f(x) = 3x - 2$:

* Calculate $f(2)$
* Calculate $f(-1)$

How does the function handle positive vs negative inputs?


### **Polynomials: Modeling Nonlinear Relationships**

Polynomials expand functions by adding powers:

| **General Polynomial**                        |
| --------------------------------------------- |
| $y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \dots$ |

Example (quadratic):

$$
y = -x^2 + 4x + 1
$$

Why is this important in trading AI?
Markets rarely behave in simple, straight-line (linear) patterns.

* Small signals might barely affect price.
* Large, dramatic signals can cause outsized reactions.

Polynomials help AI capture these nonlinear relationships, making predictions that better match real-world market behavior.

---

### Explore Polynomial Shape

Sketch or plot:

* $y = 2x + 1$ (linear)
* $y = -x^2 + 4x + 1$ (quadratic)

Where do they rise? Where do they bend?
How does adding the $x^2$ term change the behavior?


#### Compare Linear vs Polynomial

Sketch or plot:

* $y = 2x + 1$ (straight line)
* $y = -x^2 + 4x + 1$ (parabola)

Observe: Where does each rise or bend? How does adding the $x^2$ term change the shape?

---

### **Solving Linear Systems: Balancing Multiple Conditions**

AI often needs to satisfy several equations at once.

Example system:

$$
2x + 3y = 5 \\
x - y = 2
$$

This can represent balancing multiple conditions, like maximizing returns while keeping risk in check.



#### **Solve the System**

Step 1: Solve second equation for $x$:

$$
x = y + 2
$$

Step 2: Plug into the first equation:

$$
2(y + 2) + 3y = 5 \rightarrow 2y + 4 + 3y = 5 \rightarrow 5y = 1 \rightarrow y = 0.2
$$

Step 3: Find $x$:

$$
x = 0.2 + 2 = 2.2
$$

Result:

$$
x = 2.2, \quad y = 0.2
$$

Why does this matter?
AI optimization (like adjusting portfolio weights or risk factors) often comes down to solving large systems like this, sometimes with hundreds of variables.




## Linear Algebra

Linear algebra builds on basic algebra, but instead of handling single numbers, it works with:

- Vectors
- Matrices

It is often called the language of machine learning and artificial intelligence (AI) because:

* Data is usually stored as vectors and matrices
* Models use matrix operations to make predictions
* Many algorithms, such as principal component analysis (PCA), linear regression, and neural networks, rely heavily on linear algebra



## Scalars, Vectors, and Vector Operations



### 1.1 Scalars

A scalar is simply a single number.
Examples: 5, -3.2, 3.14

In the context of AI, scalars appear as bias terms, learning rates, or single numerical features.



### 1.2 Vectors

A vector is an ordered list of numbers, which can be visualized in two ways:

* Row vector: \[x1, x2, x3]
* Column vector:
  \[
  x1
  x2
  x3
  ]

Example:
Imagine you are storing details about a student:

* Height: 170 cm
* Age: 18 years
* Score: 95

This can be written as a vector:
\[170, 18, 95]

This vector belongs to three-dimensional real space, written as R³.



### 1.3 Vector Operations

Here are some basic but essential operations you can perform with vectors.



**1. Addition**
When you add two vectors, you add their corresponding elements.

Example:
\[1, 2, 3] + \[4, 5, 6] = \[5, 7, 9]



**2. Scalar Multiplication**
When you multiply a vector by a scalar, you multiply each element by that scalar.

Example:
2 \* \[3, -1] = \[6, -2]



**3. Dot Product (Inner Product)**
The dot product of two vectors is the sum of the products of their corresponding elements.

Example:
\[a1, a2, a3] • \[b1, b2, b3] = a1 \* b1 + a2 \* b2 + a3 \* b3

In AI, the dot product is often used to measure similarity between vectors, which is important in tasks like natural language processing and recommendation systems.



**4. Vector Norm (Length)**
The norm or length of a vector is calculated as:
||a|| = sqrt(a1² + a2² + ... + an²)

This gives the magnitude or size of the vector.



**5. Unit Vector**
A unit vector has a length of 1 and is obtained by dividing a vector by its norm:
â = a / ||a||

Unit vectors are often used to normalize data before feeding it into machine learning models.



## Real-World Example in Machine Learning

Imagine a machine learning dataset:

| Age | Income | Score |
| --- | ------ | ----- |
| 18  | 30000  | 72    |
| 22  | 50000  | 85    |

Each row represents a data point and can be treated as a vector.
This allows machine learning algorithms to apply mathematical operations on the dataset efficiently.



## Summary 

| Concept      | Meaning                       |
| ------------ | ----------------------------- |
| Scalar       | A single number               |
| Vector       | A list of numbers             |
| Add/Subtract | Combine or remove values      |
| Dot Product  | Measures similarity           |
| Norm         | Magnitude or length of vector |
| Unit Vector  | Vector scaled to length 1     |



## Activities for Practice

1. Write three features about yourself (for example: height, age, favorite number) and represent them as a vector.

2. Take two simple vectors, like \[1, 2, 3] and \[4, 0, -1], and calculate:

   * Their sum
   * Their dot product
   * The norm (length) of each

3. Normalize the vector \[3, 4] into a unit vector.

4. In your own words, explain where you think vectors might appear in an AI system you use every day (for example, a recommendation system or voice assistant).







## Matrices

---

### What is a Matrix?

A **matrix** is a 2-dimensional array of numbers, arranged in rows and columns.

It looks like this:

$$
A =
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}
$$

 This matrix has 2 rows and 3 columns.
 We say it has the shape \$2 \times 3\$ (read: "2 by 3").

Each number in a matrix is called an **element** or **entry**.



## 2.1 Matrix Notation

 A matrix is usually denoted by uppercase letters: \$A, B, X, W\$
 Elements are denoted as \$a\_{ij}\$, where:

   \$i\$ is the row number
   \$j\$ is the column number

For example, in:

$$
A =
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\Rightarrow a_{12} = 2, \quad a_{21} = 3
$$



## 2.2 Types of Matrices

| Type                 | Description                     |
| -------------------- | ------------------------------- |
| **Square Matrix**    | Rows = Columns                  |
| **Row Matrix**       | Only one row                    |
| **Column Matrix**    | Only one column                 |
| **Zero Matrix**      | All elements = 0                |
| **Identity Matrix**  | Diagonal = 1, others = 0        |
| **Diagonal Matrix**  | Only diagonal may have values   |
| **Symmetric Matrix** | \$A = A^T\$ (Transpose is same) |


### 1. Matrix Addition / Subtraction

Can only be done if both matrices are of the same size:

$$
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
+
\begin{bmatrix}
5 & 6 \\
7 & 8
\end{bmatrix}
= \begin{bmatrix}
6 & 8 \\
10 & 12
\end{bmatrix}
$$


### 2. Scalar Multiplication

Multiply every element of the matrix by a scalar:
$$
2 \cdot
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
= \begin{bmatrix}
2 & 4 \\
6 & 8
\end{bmatrix}
$$

---

### 3. Matrix Multiplication

If:
- A is m × n  
- B is n × p  
Then the result C = A × B is m × p

Example:

$$
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\cdot
\begin{bmatrix}
5 \\
6
\end{bmatrix}
= \begin{bmatrix}
1\cdot5 + 2\cdot6 \\
3\cdot5 + 4\cdot6
\end{bmatrix}
= \begin{bmatrix}
17 \\
39
\end{bmatrix}
$$


This is extremely important in neural networks:

**Input vector × Weight matrix = Output vector**


### 4. Matrix Transpose

Flips rows and columns:

$$
A =
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
\Rightarrow
A^T =
\begin{bmatrix}
1 & 3 \\
2 & 4
\end{bmatrix}
$$

---

### 5. Identity Matrix

A special square matrix with 1’s on the diagonal and 0’s elsewhere:

$$
I =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\Rightarrow AI = IA = A
$$

This acts like "1" for matrices, does not change the matrix when multiplied.

---

## Real-World Use of Matrices in AI

| Use Case                        | Example                                                                     |
| ------------------------------- | --------------------------------------------------------------------------- |
| **Data storage**                | Entire datasets stored as matrices (rows = data points, columns = features) |
| **Neural networks**             | Each layer applies a matrix transformation (weights × inputs)               |
| **Image processing**            | Images are 2D or 3D matrices (RGB values)                                   |
| **Natural Language Processing** | Word embeddings are stored in large matrices                                |
| **Dimensionality Reduction**    | PCA uses eigenvectors of covariance matrices                                |

---

## Summary of Part 2: Matrices

| Concept               | Use                               |
| --------------------- | --------------------------------- |
| Matrix                | Table of numbers                  |
| Matrix addition       | Add same-size matrices            |
| Scalar multiplication | Multiply every element by number  |
| Matrix multiplication | Combine multiple data features    |
| Transpose             | Swap rows and columns             |
| Identity              | Neutral element of multiplication |

---



#  Determinants and Inverse Matrices



## Why This Chapter Matters

You’ve seen the equation:

$$
Ax = b \Rightarrow x = A^{-1}b
$$

But this **only works** if:

 \$A\$ is **square** (same number of rows and columns)
 \$A\$ is **invertible**

To **check** if a matrix is invertible, we calculate its **determinant**.



## 4.1 Determinant

The **determinant** of a square matrix is a **single number** that gives information about:

* Whether a matrix is invertible
* The volume scaling factor of a transformation
* If a set of vectors is linearly dependent



### Determinant of a 2×2 Matrix

$$
A =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\Rightarrow
\det(A) = ad - bc
$$

Example:

$$
\begin{bmatrix}
2 & 3 \\
4 & 5
\end{bmatrix}
\Rightarrow \det = (2)(5) - (3)(4) = 10 - 12 = -2
$$

If **det ≠ 0**, the matrix is **invertible**.



### Determinant of a 3×3 Matrix

Use the **rule of Sarrus** or **cofactor expansion**:


> $$
> \begin{vmatrix}
> a & b & c\\
> d & e & f\\
> g & h & i
> \end{vmatrix}
> = a\,(ei - fh)\;-\;b\,(di - fg)\;+\;c\,(dh - eg)
> $$

 **Example:**
$$
\begin{vmatrix}
1 & 2 & 3 \\
0 & 1 & 4 \\
5 & 6 & 0
\end{vmatrix}
= 1(1×0 - 4×6) - 2(0×0 - 4×5) + 3(0×6 - 1×5)
= 1(0 - 24) - 2(0 - 20) + 3(0 - 5)
= -24 + 40 - 15 = 1
$$



### Key Rule

| Determinant | Matrix Type                   | System                   |
| ----------- | ----------------------------- | ------------------------ |
| 0           | **Not invertible** (Singular) | No or infinite solutions |
| ≠ 0         | **Invertible** (Non-singular) | Unique solution exists   |

---

## 4.2 Inverse of a Matrix

If \$A\$ is a square matrix and \$\det(A) \ne 0\$, then there exists a matrix \$A^{-1}\$ such that:

$$
AA^{-1} = A^{-1}A = I
$$

---

### Inverse of a 2×2 Matrix

$$
A =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\Rightarrow
A^{-1} =
\frac{1}{ad - bc}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
$$

---



---

### If \$\det = 0\$

* The matrix **cannot be inverted**
* The system \$Ax = b\$ has **no unique solution**
* It may be **dependent** or **inconsistent**

---

## 4.3 Application in AI / ML

| Concept              | How Determinants and Inverses Matter                                       |
| -------------------- | -------------------------------------------------------------------------- |
| Linear regression    | Use inverse to compute weights: \$w = (X^TX)^{-1}X^Ty\$                    |
| Neural networks      | Rarely use inverses directly, but matrix invertibility ensures solvability |
| Data transformations | Determinant shows if transformation flips or scales data                   |
| PCA / SVD            | Use determinant to analyze covariance matrices                             |

---

## Summary: Determinants and Inverses

| Term        | Meaning                                 |
| ----------- | --------------------------------------- |
| Determinant | A number that indicates invertibility   |
| Invertible  | \$\det \ne 0\$, unique solution exists  |
| Singular    | \$\det = 0\$, no inverse                |
| Inverse     | "Undo" of a matrix                      |
| Use         | Solving \$Ax = b\$, regression, ML math |




---

### Think and Discuss

**Why do you think using matrices and linear algebra is helpful when working with large datasets?**

Possible answers:

* It allows efficient computation.
* It helps detect patterns across many variables.
* It scales well when data grows.

---

### **Which is the hardest point for the AI linear model?**


| **Sentiment Score (x)** | **Actual Price Change (y%)** |
| ----------------------- | ---------------------------- |
| 0.3                     | 1.2                          |
| -0.5                    | -3.8                         |
| 0.8                     | 4.1                          |
| -0.9                    | -10.0                        |
| 0.6                     | 3.5                          |
| 0.1                     | 0.7                          |
| -0.7                    | -2.5                         |
| 1.0                     | 6.0                          |


Look at:

* Most points follow a **roughly proportional** change (small sentiment → small price shift).
* But **(-0.9, –10.0)** stands out as a **sharp drop**, far steeper than other negative sentiment points.

This makes **(-0.9, –10.0)** the likely **outlier**:
-Much steeper negative price drop.
-Possibly influenced by non-linear effects or sudden market panic.
-Linear models (which fit lines) struggle with such **sharp bends**.

### Summary

In summary:

* **Algebra** gives AI the equations to describe relationships in data.
* **Linear algebra** provides the tools to learn these equations efficiently.

With these tools, AI can turn simple sentiment scores into powerful predictions that help traders like Alex make informed decisions.

